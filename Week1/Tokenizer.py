# -*- coding: utf-8 -*-
"""Tokenizer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6VPYbI8A4idk4GlwmlBB9Ns3U2dOczm

###Tokenizer

To work with words we have to assign to encode each word with a number.There are different APIS to do so, one of them is the Tokenizer.. The Tokenizer generates a dictionary of word encodings, and creates vectors out of the sentences given.


*   The **num_words** parameter is the number of  words to it.
*   **The tokenizer strips punctuation out**.  The parameter **filters** is a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character.
*   By setting the **lower == True** the texts submitted are converted to lowercase.
*   The parameter **split** is the separator for word splitting which can be also configured.
*   **char_level:** If True, every character will be treated as a token. 
*   **oov_token**	If given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls
"""

tf.keras.preprocessing.text.Tokenizer(
    num_words=None,
    filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True, split=' ', char_level=False, oov_token=None,
    document_count=0, **kwargs
)

"""**Example 1**:

In this example, 100 is the number of words choosen, by setting this hyperparameter, what the tokenizer will do is take the top 100 words by volume and just encode those. It's a handy shortcut when dealing with lots of data, and worth experimenting with when you train with real data.  Sometimes the impact of less words can be minimal and training accuracy, but huge in training time.

The tokenizer provides a **word index property** which returns a dictionary containing key value pairs, where the key is the word, and the value is the token for that word, which you can inspect by simply printing it out.

The **fit on texts** method of the tokenizer then takes in the data and encodes it. The tokenizer provides a word index property which returns a dictionary containing key value pairs, where the key is the word, and the value is the token for that word,.
"""

from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
    'i love my dog',
    'I, love my cat',
    'You love my dog!'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(word_index)

"""##References


*   Tensorflow API Documents [Tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)
*   The examples are from the course [Natural Language Processing](https://www.coursera.org/learn/natural-language-processing-tensorflow/) from Coursera. 




"""
