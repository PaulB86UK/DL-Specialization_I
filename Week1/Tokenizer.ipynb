{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenizer",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIanREwppByy"
      },
      "source": [
        "###Tokenizer\n",
        "\n",
        "Generating the dictionary of word encodings and creating vectors out of the sentences. \n",
        "\n",
        "\n",
        "\n",
        "*   The **num_words** parameter is the number of  words to it.\n",
        "*   **The tokenizer strips punctuation out**.  The parameter **filters** is a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character.\n",
        "*   By setting the **lower == True** the texts submitted are converted to lowercase.\n",
        "*   The parameter **split** is the separator for word splitting which can be also configured.\n",
        "*   **char_level:** If True, every character will be treated as a token. \n",
        "*   **oov_token**\tIf given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls\n",
        "\n",
        "[Reference Tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LirHJx53zGBY"
      },
      "source": [
        "tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True, split=' ', char_level=False, oov_token=None,\n",
        "    document_count=0, **kwargs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4H4f4Hbzl5V"
      },
      "source": [
        "The following are examples from the course [natural-language-processing-tensorflow](https://www.coursera.org/learn/natural-language-processing-tensorflow/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMm5lS3vze7C"
      },
      "source": [
        "**Example 1**:\n",
        "\n",
        "In this example, 100 is the number of words choosen, by setting this hyperparameter, what the tokenizer will do is take the top 100 words by volume and just encode those. It's a handy shortcut when dealing with lots of data, and worth experimenting with when you train with real data.  Sometimes the impact of less words can be minimal and training accuracy, but huge in training time.\n",
        "\n",
        "The tokenizer provides a **word index property** which returns a dictionary containing key value pairs, where the key is the word, and the value is the token for that word, which you can inspect by simply printing it out.\n",
        "\n",
        "The **fit on texts** method of the tokenizer then takes in the data and encodes it. The data used here is the list created under the variable sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaCMcjMQifQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cff979-8ebf-49a8-bb75-da04336213e1"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTj-IXJG1QZU"
      },
      "source": [
        ""
      ]
    }
  ]
}
